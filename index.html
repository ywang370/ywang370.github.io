<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Yilin Wang - Research Scientist at Adobe specializing in computer vision, image editing, and deep learning. Primary contributor to Photoshop's Select Subject, Object Finder, and Select People Details features.">
  <meta name="keywords" content="Yilin Wang, Adobe, Research Scientist, Computer Vision, Deep Learning, Image Editing, Photoshop, Machine Learning">
  <meta name="author" content="Yilin Wang">
  
  <!-- Open Graph / Social Media -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="Yilin Wang - Research Scientist at Adobe">
  <meta property="og:description" content="Research Scientist at Adobe specializing in computer vision, image editing, and deep learning.">
  <meta property="og:image" content="image/new_profile1.JPG">
  <meta property="og:url" content="https://ywang370.github.io">
  <meta name="twitter:card" content="summary_large_image">
  
  <title>Yilin Wang - Research Scientist at Adobe</title>
  <link rel="icon" type="image/png" href="favicon.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.0/font/bootstrap-icons.css">
  <style>
    :root {
      --text-primary: #1a1a2e;
      --text-secondary: #4a4a68;
      --text-muted: #6b7280;
      --accent: #2563eb;
      --accent-hover: #1d4ed8;
      --accent-light: #eff6ff;
      --bg-page: #fafbfd;
      --bg-card: #ffffff;
      --bg-highlight: #fffef0;
      --border-light: #e5e7eb;
      --border-highlight: #f59e0b;
      --shadow-sm: 0 1px 3px rgba(0,0,0,0.04), 0 1px 2px rgba(0,0,0,0.06);
      --shadow-md: 0 4px 12px rgba(0,0,0,0.07), 0 1px 3px rgba(0,0,0,0.05);
      --shadow-lg: 0 10px 25px rgba(0,0,0,0.08), 0 2px 6px rgba(0,0,0,0.04);
      --radius: 10px;
    }

    html { scroll-behavior: smooth; }
    
    body { 
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif; 
      line-height: 1.65; 
      background-color: var(--bg-page); 
      color: var(--text-primary); 
      font-size: 15px;
      -webkit-font-smoothing: antialiased;
    }
    
    .container { max-width: 1060px; margin: auto; padding: 0 2rem; }

    /* ─── Navigation ─── */
    .site-nav {
      background: rgba(255,255,255,0.85);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      border-bottom: 1px solid var(--border-light);
      padding: 0.6rem 0;
      z-index: 100;
    }
    .site-nav .nav-link {
      color: var(--text-secondary);
      font-size: 0.88rem;
      font-weight: 500;
      padding: 0.4rem 1rem;
      border-radius: 8px;
      margin: 0 0.15rem;
      transition: all 0.2s ease;
    }
    .site-nav .nav-link:hover { background: var(--accent-light); color: var(--accent); }
    .site-nav .nav-link.active { background: var(--accent); color: #fff; }

    /* ─── Profile ─── */
    .profile-section { 
      display: flex; align-items: center; gap: 2.5rem; 
      padding: 2.5rem 0 1.5rem;
    }
    .profile-img { 
      width: 220px; height: auto; border-radius: 12px; object-fit: cover; flex-shrink: 0; 
      box-shadow: var(--shadow-lg);
      border: 3px solid #fff;
    }
    .profile-text { flex: 1; }
    .profile-text h1 { 
      margin: 0 0 0.15rem; font-size: 2rem; font-weight: 700; 
      color: var(--text-primary); letter-spacing: -0.02em;
    }
    .profile-text .lead { 
      margin-bottom: 0.6rem; font-size: 1.05rem; font-weight: 400; 
      color: var(--text-secondary); 
    }
    .profile-text .lead a { color: var(--accent); text-decoration: none; font-weight: 500; }
    .profile-text .lead a:hover { text-decoration: underline; }
    .profile-bio { 
      font-size: 0.92rem; color: var(--text-secondary); line-height: 1.7; 
      margin-bottom: 0.8rem;
    }
    .profile-bio a { color: var(--accent); text-decoration: none; }
    .profile-bio a:hover { text-decoration: underline; }

    .contact-links { margin-top: 0.6rem; display: flex; flex-wrap: wrap; gap: 0.5rem; }
    .contact-links a { 
      padding: 0.4rem 0.9rem; border-radius: 8px; font-size: 0.85rem; font-weight: 500;
      background: var(--accent-light); text-decoration: none; color: var(--accent); 
      transition: all 0.2s ease; display: inline-flex; align-items: center; gap: 0.35rem;
    }
    .contact-links a:hover { 
      background: var(--accent); color: #fff; 
      box-shadow: 0 2px 8px rgba(37,99,235,0.3); transform: translateY(-1px); 
    }

    /* ─── Sections ─── */
    section { padding: 0.5rem 0; }
    h2 { 
      font-size: 1.3rem; font-weight: 700; color: var(--text-primary);
      margin-top: 2rem; margin-bottom: 1rem; padding-bottom: 0.6rem;
      border-bottom: 2px solid var(--border-light);
      scroll-margin-top: 70px;
      letter-spacing: -0.01em;
    }
    h2 i { color: var(--accent); margin-right: 0.3rem; }

    /* ─── Hiring Banner ─── */
    .hiring-banner {
      background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
      color: white; padding: 0.85rem 1.2rem; border-radius: var(--radius);
      margin: 1rem 0 0; font-weight: 500; font-size: 0.9rem;
      box-shadow: 0 2px 10px rgba(239,68,68,0.2);
    }
    .hiring-banner a { color: #fef2f2; font-weight: 600; }

    /* ─── News ─── */
    .news-list { list-style: none; padding-left: 0; margin: 0; }
    .news-list li { 
      padding: 0.55rem 0; border-bottom: 1px solid var(--border-light); 
      font-size: 0.9rem; color: var(--text-secondary);
    }
    .news-list li:last-child { border-bottom: none; }
    .news-date { 
      font-weight: 600; color: var(--text-primary); min-width: 100px; 
      display: inline-block; font-size: 0.88rem;
    }
    .news-list a { color: var(--accent); text-decoration: none; }
    .news-list a:hover { text-decoration: underline; }

    /* ─── Paper Entries ─── */
    .red-star { color: #ef4444; font-weight: 600; }

    .paper-entry { 
      display: flex; align-items: flex-start; gap: 1.5rem;
      margin-bottom: 1.2rem; padding: 1.2rem; 
      background: var(--bg-card); border-radius: var(--radius); 
      box-shadow: var(--shadow-sm); border: 1px solid var(--border-light);
      transition: all 0.25s ease;
    }
    .paper-entry:hover { 
      box-shadow: var(--shadow-md); transform: translateY(-2px); 
      border-color: #d1d5db;
    }
    .highlighted-paper { 
      background: var(--bg-highlight); 
      border-left: 4px solid var(--border-highlight);
      border-color: var(--border-highlight) var(--border-light) var(--border-light);
    }
    .highlighted-paper:hover { border-color: var(--border-highlight) #d1d5db #d1d5db; }

    .paper-entry .paper-thumb { 
      width: 250px; min-width: 250px; border-radius: 8px; object-fit: cover;
      box-shadow: 0 1px 4px rgba(0,0,0,0.08);
    }
    .paper-details { flex: 1; min-width: 0; }
    .paper-details h5 { 
      margin-bottom: 0.35rem; font-size: 0.95rem; font-weight: 600; 
      line-height: 1.4; color: var(--text-primary);
    }
    .paper-details h5 a { color: var(--accent); text-decoration: none; }
    .paper-details h5 a:hover { text-decoration: underline; color: var(--accent-hover); }

    .paper-venue { 
      display: inline-block; font-size: 0.78rem; font-weight: 600; 
      color: var(--accent); background: var(--accent-light); 
      padding: 0.15rem 0.6rem; border-radius: 5px; margin-bottom: 0.4rem;
      letter-spacing: 0.01em;
    }

    .paper-meta { 
      font-size: 0.84rem; color: var(--text-muted); margin-bottom: 0.35rem; 
      line-height: 1.55;
    }
    .paper-meta a { color: var(--text-secondary); text-decoration: none; }
    .paper-meta a:hover { color: var(--accent); text-decoration: underline; }
    .paper-meta strong { color: var(--text-primary); }

    .paper-links { font-size: 0.84rem; margin-bottom: 0.3rem; }
    .paper-links a { 
      color: var(--accent); text-decoration: none; font-weight: 500;
      padding: 0.15rem 0.5rem; border-radius: 4px;
      transition: background 0.15s ease;
    }
    .paper-links a:hover { background: var(--accent-light); }

    .paper-desc { 
      font-size: 0.87rem; color: var(--text-secondary); margin: 0.25rem 0 0; 
      line-height: 1.6;
    }

    /* ─── PhD Research ─── */
    .paper-year { display: flex; gap: 2rem; margin-bottom: 1rem; }
    .paper-year .institution-logo { flex-shrink: 0; padding-top: 0.3rem; }
    .paper-year .paper-details { flex: 1; }
    .paper-year h4 { 
      color: var(--accent); margin-top: 1.5rem; margin-bottom: 0.6rem; 
      font-weight: 700; font-size: 1.05rem; letter-spacing: -0.01em;
    }
    .paper-year h4:first-child { margin-top: 0; }
    .paper-year ul { list-style: none; padding-left: 0; }
    .paper-year li {
      margin-bottom: 0.7rem; padding: 0.7rem 0.9rem; 
      background: var(--bg-card); border-radius: 8px; 
      border: 1px solid var(--border-light);
      font-size: 0.88rem; line-height: 1.6; color: var(--text-secondary);
      transition: all 0.2s ease;
    }
    .paper-year li:hover { box-shadow: var(--shadow-sm); border-color: #d1d5db; }
    .paper-year li i { color: var(--text-primary); font-style: italic; }
    .paper-year li a { color: var(--accent); text-decoration: none; font-weight: 500; }
    .paper-year li a:hover { text-decoration: underline; }

    /* ─── Highlight badges ─── */
    .highlight {
      background: linear-gradient(135deg, #fbbf24, #f59e0b); 
      color: #78350f; padding: 0.12rem 0.5rem;
      border-radius: 4px; font-size: 0.76rem; font-weight: 600;
      letter-spacing: 0.01em;
    }

    /* ─── Service ─── */
    #service ul { padding-left: 1.2rem; }
    #service li { 
      margin-bottom: 0.5rem; font-size: 0.9rem; color: var(--text-secondary); 
      line-height: 1.65;
    }
    #service li strong { color: var(--text-primary); }
    #service li a { color: var(--accent); text-decoration: none; }
    #service li a:hover { text-decoration: underline; }

    /* ─── Footer ─── */
    footer { 
      margin-top: 3rem; padding: 1.5rem 0; 
      border-top: 1px solid var(--border-light); 
    }
    footer p { color: var(--text-muted); font-size: 0.82rem; }
    footer a { color: var(--accent); text-decoration: none; }
    footer a:hover { text-decoration: underline; }

    /* ─── Back to Top ─── */
    .back-to-top {
      position: fixed; bottom: 28px; right: 28px; width: 42px; height: 42px;
      border-radius: 10px; background: var(--accent); color: white; border: none;
      cursor: pointer; opacity: 0; visibility: hidden; transition: all 0.3s ease;
      z-index: 1000; box-shadow: 0 4px 12px rgba(37,99,235,0.3); font-size: 1rem;
      display: flex; align-items: center; justify-content: center;
    }
    .back-to-top.visible { opacity: 1; visibility: visible; }
    .back-to-top:hover { background: var(--accent-hover); transform: translateY(-2px); }

    /* ─── Responsive ─── */
    @media (max-width: 768px) {
      .container { padding: 0 1rem; }
      .profile-section { flex-direction: column; text-align: center; gap: 1.5rem; padding: 1.5rem 0 1rem; }
      .profile-img { width: 180px; }
      .contact-links { justify-content: center; }
      .paper-entry { flex-direction: column; }
      .paper-entry .paper-thumb { width: 100%; min-width: unset; }
      .paper-year { flex-direction: column; gap: 1rem; }
      .paper-year .institution-logo { text-align: center; }
    }
  </style>
</head>
<body>
  <!-- Skip to main content for accessibility -->
  <a href="#main-content" class="visually-hidden-focusable">Skip to main content</a>
  
  <!-- Navigation -->
  <nav class="sticky-top site-nav">
    <div class="container">
      <ul class="nav justify-content-center flex-wrap">
        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
        <li class="nav-item"><a class="nav-link" href="#news">News</a></li>
        <li class="nav-item"><a class="nav-link" href="#research">Research</a></li>
        <li class="nav-item"><a class="nav-link" href="#phd">PhD Work</a></li>
        <li class="nav-item"><a class="nav-link" href="#service">Service</a></li>
      </ul>
    </div>
  </nav>

  <div class="container" id="main-content">
    <!-- Profile Section -->
    <section id="about">
      <div class="profile-section">
        <img src="image/new_profile1.JPG" alt="Yilin Wang - Research Scientist at Adobe" class="profile-img" loading="lazy">
        <div class="profile-text">
          <h1>Yilin Wang</h1>
          <p class="lead">Research Scientist at <a href="https://www.adobe.com/">Adobe</a>, San Jose</p>
          <p class="profile-bio">
            I work on computer vision for Imaging products at Adobe. Primary contributor to
            <a href="https://firefly.adobe.com">Instruction-based Image Editing</a>,
            <a href="https://theblog.adobe.com/photoshop-reimagined-for-ipad-introducing-select-subject/">Select Subject</a>,
            <a href="https://photoshopcafe.com/use-amazing-new-object-finder-photoshop-2022-instant-automatic-selections-objects-image/">Object Finder</a>, and
            <a href="https://fstoppers.com/photoshop/master-precise-adjustments-photoshops-latest-feature-688289">Select People Details</a>.
            Ph.D. from <a href="https://www.asu.edu/">Arizona State University</a>, advised by <a href="https://search.asu.edu/profile/747601">Baoxin Li</a>.
            8 tech transfers to Photoshop, Lightroom, and Stardust.
          </p>
          <div class="contact-links">
            <a href="mailto:wangyilin930@gmail.com"><i class="bi bi-envelope"></i> Email</a>
            <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ&hl=en"><i class="bi bi-mortarboard"></i> Scholar</a>
            <a href="https://www.linkedin.com/in/yilinasu/"><i class="bi bi-linkedin"></i> LinkedIn</a>
            <a href="https://github.com/ywang370"><i class="bi bi-github"></i> GitHub</a>
          </div>
        </div>
      </div>

      <div class="hiring-banner">
        <i class="bi bi-megaphone"></i> Now recruiting summer research interns in image/video editing with MLLM. <a href="mailto:wangyilin930@gmail.com" class="text-white text-decoration-underline">Contact me!</a>
      </div>
    </section>

    <!-- News Section -->
    <section id="news">
      <h2><i class="bi bi-newspaper"></i> News</h2>
      <ul class="news-list">
        <li><span class="news-date">Feb. 2026</span> Firefly Imagen5 Instruction-based Image Editing released! (core contributor)</li>
        <li><span class="news-date">Feb. 2025</span> 3 papers accepted to CVPR 2025</li>
        <li><span class="news-date">Jan. 2025</span> 1 paper accepted to AAAI 2025</li>
        <li><span class="news-date">Nov. 2024</span> "Select People Details" featured by <a href="https://www.youtube.com/watch?v=8vO-9JT9MU0">Colin Smith on YouTube</a></li>
      </ul>
    </section>

    <!-- Research Section -->
    <section id="research">
      <h2><i class="bi bi-journal-richtext"></i> Highlighted Research</h2>
      <p style="color: var(--text-secondary); font-size: 0.92rem;"><strong style="color: var(--text-primary);">I strive for simple yet scalable methods</strong> in image understanding and editing. Representative works are highlighted below. Full list on <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ&hl=en" style="color: var(--accent); text-decoration: none; font-weight: 500;">Google Scholar</a>.</p>
      
      <div class="paper-entry highlighted-paper">
        <img src="images/thumbnail_xplanner.png" alt="X-Planner" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2507.05259">Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing</a></h5>
          <span class="paper-venue">AAAI 2026</span>
          <p class="paper-meta"><a href="https://danielchyeh.github.io/#">Chun-Hsiao Yeh</a>, <strong>Yilin Wang</strong>, Nanxuan Zhao, Richard Zhang, Yuheng Li, Yi Ma, Krishna Kumar Singh</p>
          <p class="paper-links"><a href="https://danielchyeh.github.io/x-planner/">project page</a> / <a href="https://arxiv.org/abs/2507.05259">paper</a></p>
          <p class="paper-desc">An MLLM-based planning system that bridges user intent with editing model capabilities.</p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/oministyle.gif" alt="OmniStyle" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2505.14028">OmniStyle: Filtering High Quality Style Transfer Data at Scale</a></h5>
          <span class="paper-venue">CVPR 2025</span>
          <p class="paper-meta"><a href="https://wangyephd.github.io/">Ye Wang</a>, Ruiqi Liu, Jiang Lin, <a href="https://is.nju.edu.cn/yzl_en/main.htm">Zili Yi</a>, <strong>Yilin Wang<span class="red-star">★</span></strong>, <a href="https://ruim-jlu.github.io/#about">Rui Ma<span class="red-star">★</span></a> &nbsp;<span class="red-star">★</span>co-advisor</p>
          <p class="paper-links"><a href="https://wangyephd.github.io/projects/cvpr25_omnistyle.html">project page</a> / <a href="https://arxiv.org/abs/2505.14028">paper</a></p>
          <p class="paper-desc">First end-to-end style transfer framework on DiT, achieving 1K-resolution stylization with the OmniStyle-1M dataset. Supports both instruction- and image-guided stylization.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/finecap.svg" alt="FineCaption" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2312.14985">FINECAPTION: Compositional Image Captioning</a></h5>
          <span class="paper-venue">CVPR 2025</span>
          <p class="paper-meta">Hang Hua, <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en">Qing Liu</a>, Lingzhi Zhang, Jing Shi, Zhifei Zhang, <strong>Yilin Wang</strong>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, Jiebo Luo, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a></p>
          <p class="paper-desc">A unified VLM for free-form mask grounding and compositional captioning.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/UniReal.gif" alt="UniReal" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2412.07774">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</a></h5>
          <span class="paper-venue">ICLR 2025 (Highlight)</span>
          <p class="paper-meta">Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, <strong>Yilin Wang</strong>, Hui Ding, Zhe Lin, Hengshuang Zhao</p>
          <p class="paper-links"><a href="https://xavierchen34.github.io/UniReal-Page/">project page</a> / <a href="https://arxiv.org/abs/2412.07774">pdf</a></p>
          <p class="paper-desc"><span style="color: #ef4444; font-weight: 600;">Foundational multi-modal generative model</span> — A universal framework for image generation and editing by treating input/output images as video frames, learning real-world dynamics from video supervision.</p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/sig_style1.jpg" alt="SigStyle" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="http://arxiv.org/abs/2404.05717">SigStyle: Signature Style Transfer via Personalized Text-to-Image Models</a></h5>
          <span class="paper-venue">AAAI 2025</span>
          <p class="paper-meta"><a href="https://wangyephd.github.io/">Ye Wang</a>, Tongyuan Bai, Xuping Xie, <a href="https://is.nju.edu.cn/yzl_en/main.htm">Zili Yi</a>, <strong>Yilin Wang<span class="red-star">★</span></strong>, <a href="https://ruim-jlu.github.io/#about">Rui Ma<span class="red-star">★</span></a> &nbsp;<span class="red-star">★</span>co-advisor</p>
          <p class="paper-links"><a href="https://wangyephd.github.io/projects/sigstyle.html">project page</a> / <a href="http://arxiv.org/abs/2404.05717">paper</a></p>
          <p class="paper-desc">Style-preserved transfer via personalized subject editing diffusion model.</p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/swapanything_after.png" alt="SwapAnything" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="http://arxiv.org/abs/2404.05717">SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing</a></h5>
          <span class="paper-venue">ECCV 2024</span>
          <p class="paper-meta"><a href="https://g-jing.github.io/">Jing Gu</a>, <a href="http://nxzhao.com/">Nanxuan Zhao</a>, <a href="https://wxiong.me/">Wei Xiong</a>, <a href="https://qliu24.github.io/">Qing Liu</a>, <a href="https://zzutk.github.io/">Zhifei Zhang</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>, <strong>Yilin Wang</strong><span class="red-star">★</span>, <a href="https://eric-xw.github.io/">Xin Eric Wang</a><span class="red-star">★</span> &nbsp;<span class="red-star">★</span>Co-advisor</p>
          <p class="paper-links"><a href="https://swap-anything.github.io/">project page</a> / <a href="http://arxiv.org/abs/2404.05717">paper</a></p>
          <p class="paper-desc">Personalized subject-driven image editing with arbitrary object swapping.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/unihuman.png" alt="UniHuman" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2312.14985">UniHuman: A Unified Model for Editing Human Images in the Wild</a></h5>
          <span class="paper-venue">CVPR 2024</span>
          <p class="paper-meta">Nannan Li, <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en">Qing Liu</a>, <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>, <strong>Yilin Wang</strong>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, Bryan A. Plummer, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a></p>
          <p class="paper-desc">Human editing via diffusion.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/amodal.png" alt="Amodal Segmentation" class="paper-thumb">
        <div class="paper-details">
          <h5>Amodal Scene Analysis via Holistic Occlusion Relation Inference and Generative Mask Completion</h5>
          <span class="paper-venue">AAAI 2024 (oral)</span>
          <p class="paper-meta">Bowen Zhang, <a href="https://sites.google.com/site/hezhangsprinter/">Qing Liu</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <strong>Yilin Wang</strong>, Akide Liu, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>, <a href="https://scholar.google.com/citations?user=ksQ4JnQAAAAJ&hl=zh-CN">Yifan Liu</a></p>
          <p class="paper-desc">Amodal segmentation with mutual occlusion reasoning.</p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/photoswap_after.png" alt="PhotoSwap" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2305.18286">PHOTOSWAP: Personalized Subject Swapping in Images</a></h5>
          <span class="paper-venue">NeurIPS 2023</span>
          <p class="paper-meta"><a href="https://g-jing.github.io/">Jing Gu</a>, <strong>Yilin Wang</strong>, <a href="http://nxzhao.com/">Nanxuan Zhao</a>, <a href="https://tsujuifu.github.io/">Tsu-Jui Fu</a>, <a href="https://wxiong.me/">Wei Xiong</a>, <a href="https://qliu24.github.io/">Qing Liu</a>, <a href="https://zzutk.github.io/">Zhifei Zhang</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>, <a href="https://eric-xw.github.io/">Xin Eric Wang</a></p>
          <p class="paper-links"><a href="https://photoswap.github.io/">project page</a> / <a href="https://arxiv.org/pdf/2305.18286.pdf">paper</a></p>
          <p class="paper-desc">Personalized subject-driven image editing.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/lightpainter.png" alt="LightPainter" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2303.12950">LightPainter: Interactive Portrait Relighting with Freehand Scribble</a></h5>
          <span class="paper-venue">CVPR 2023</span>
          <p class="paper-meta"><a href="https://yiqunmei.net/">Yiqun Mei</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <a href="https://zhixinshu.github.io/">Zhixin Shu</a>, <strong>Yilin Wang</strong>, <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>, Yan Shi, <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>, <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en">Vishal M. Patel</a></p>
          <p class="paper-links"><a href="https://yiqunmei.net/lightpt/">project page</a> / <a href="https://yiqunmei.net/lightpt/resources/CVPR2023_LightPainter.pdf">paper</a></p>
          <p class="paper-desc">Scribble-based interactive portrait relighting system.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/interactive.png" alt="Interactive Portrait Harmonization" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2203.08216">Interactive Portrait Harmonization</a></h5>
          <span class="paper-venue">ICLR 2023</span>
          <p class="paper-meta"><a href="https://jeya-maria-jose.github.io/research/">Jeya Maria Jose Valanarasu</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <strong>Yilin Wang</strong>, <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>, <a href="https://www.linkedin.com/in/yinglan-ma/">Yinglan Ma</a>, <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>, <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en">Vishal M. Patel</a></p>
          <p class="paper-links"><a href="https://jeya-maria-jose.github.io/IPH-web/">project page</a> / <a href="https://arxiv.org/abs/2203.08216">paper</a></p>
          <p class="paper-desc">Interactive harmonization for portrait photos.</p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/lvt.png" alt="Lite Vision Transformer" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2112.10809">Lite Vision Transformer with Enhanced Self-Attention</a></h5>
          <span class="paper-venue">CVPR 2022</span>
          <p class="paper-meta"><a href="https://www.chenglinyang.com/">Chenglin Yang</a>, <strong>Yilin Wang</strong>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>, <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>, <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></p>
          <p class="paper-links"><a href="https://www.chenglinyang.com/publication/lvt/">project page</a> / <a href="https://arxiv.org/abs/2112.10809">paper</a></p>
          <p class="paper-desc">Light-weight vision transformer models for vision tasks.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/ssh.png" alt="SSH" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2108.06805">SSH: A Self-Supervised Framework for Image Harmonization</a></h5>
          <span class="paper-venue">ICCV 2021</span>
          <p class="paper-meta"><a href="https://yifanjiang19.github.io/">Yifan Jiang</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <strong>Yilin Wang</strong>, <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>, <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, Simon Chen, <a href="https://scholar.google.com/citations?user=aFrtZOIAAAAJ&hl=en">Sohrab Amirghodsi</a>, <a href="https://blog.adobe.com/en/publish/2017/08/02/women-in-technology-sarah-kong">Sarah Kong</a>, <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Zhangyang Wang</a></p>
          <p class="paper-links"><a href="https://arxiv.org/abs/2108.06805">paper</a></p>
          <p class="paper-desc">Image harmonization based on self-supervised learning.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/yu2020mask.png" alt="Mask Guided Matting" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/pdf/2012.06722.pdf">Mask Guided Matting via Progressive Refinement Network</a></h5>
          <span class="paper-venue">CVPR 2021</span>
          <p class="paper-meta"><a href="https://yucornetto.github.io/">Qihang Yu</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>, <strong>Yilin Wang</strong>, <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>, <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>, <a href="https://yutongbai.com/">Yutong Bai</a>, <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></p>
          <p class="paper-links"><a href="https://github.com/yucornetto/MGMatting">project page</a> / <a href="https://arxiv.org/pdf/2012.06722.pdf">paper</a></p>
          <p class="paper-desc">Mask guided image matting.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/mct.png" alt="Multimodal Contrastive Training" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf">Multimodal Contrastive Training for Visual Representation Learning</a></h5>
          <span class="paper-venue">CVPR 2021</span>
          <p class="paper-meta"><a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Xin Yuan</a>, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>, <a href="https://scholar.google.com.my/citations?user=e6u7GlQAAAAJ&hl=en">Jason Kuen</a>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <strong>Yilin Wang</strong>, Michael Maire, Ajinkya Kale, Baldo Faieta</p>
          <p class="paper-links"><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf">paper</a></p>
          <p class="paper-desc">Intra- and inter-modal similarity preservation for multimodal representation learning.</p>
        </div>
      </div>

      <div class="paper-entry">
        <img src="images/RAL.png" alt="RAL" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/pdf/2007.09923">Incorporating Reinforced Adversarial Learning in Autoregressive Image Generation</a></h5>
          <span class="paper-venue">ECCV 2020</span>
          <p class="paper-meta"><a href="https://scholar.google.com.sg/citations?user=1SuLOuAAAAAJ&hl=en">Kenan E. Ak</a>, <a href="https://sites.google.com/view/ningxu/homepage">Ning Xu</a>, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>, <strong>Yilin Wang</strong></p>
          <p class="paper-links"><a href="https://arxiv.org/pdf/2007.09923">paper</a></p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/shape.png" alt="Shape Adaptor" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/pdf/2007.09923">Shape Adaptor: A Learnable Resizing Module</a></h5>
          <span class="paper-venue">ECCV 2020</span>
          <p class="paper-meta"><a href="https://shikun.io/">Shikun Liu</a>, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>, <strong>Yilin Wang</strong>, <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>, <a href="https://fperazzi.github.io/">Federico Perazzi</a>, <a href="https://scholar.google.co.uk/citations?user=dHec-LkAAAAJ&hl=en">Edward Johns</a></p>
          <p class="paper-links"><a href="https://arxiv.org/pdf/2007.09923">paper</a></p>
        </div>
      </div>

      <div class="paper-entry highlighted-paper">
        <img src="images/mmc.png" alt="Multimodal Style Transfer" class="paper-thumb">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/1904.04443">Multimodal Style Transfer via Graph Cuts</a></h5>
          <span class="paper-venue">ICCV 2019</span>
          <p class="paper-meta"><a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Yulun Zhang</a>, <a href="https://scholar.google.com.my/citations?user=e6u7GlQAAAAJ&hl=en">Chen Fang</a>, <strong>Yilin Wang</strong>, <a href="https://sites.google.com/site/zhelin625/">Zhaowen Wang</a>, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>, <a href="https://www1.ece.neu.edu/~yunfu/">Yun Fu</a>, <a href="https://jimeiyang.github.io/">Jimei Yang</a></p>
          <p class="paper-links"><a href="https://arxiv.org/abs/1904.04443">paper</a></p>
        </div>
      </div>
    

    </section>

    <!-- PhD Research Section -->
    <section id="phd">
      <h2><i class="bi bi-mortarboard-fill"></i> PhD Research</h2>
    
    <div class="paper-year">
      <div class="institution-logo">
        <img src="images/asu.avif" alt="Arizona State University" style="width:100%; max-width:140px; border-radius:8px;">
      </div>
      <div class="paper-details">
        <h4>2018</h4>
        <ul>
          <li><i>Generalizing Graph Matching beyond Quadratic Assignment Model</i><br>
            Tianshu Yu, Junchi Yan, <strong>Yilin Wang</strong>, Wei Liu, Baoxin Li<br>
            NeurIPS 2018
          </li>
          <li><i>Weakly Supervised Facial Attribute Manipulation via Deep Adversarial Network</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Guojun Qi, Jiliang Tang, Baoxin Li<br>
            WACV 2018 <a href="papers/wacv18.pdf">[paper]</a>
          </li>
          <li><i>CrossFire: Cross Media Joint Friend and Item Recommendations</i><br>
            Kai Shu, Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            WSDM 2018 <span class="highlight">spotlight</span> <a href="https://dl.acm.org/citation.cfm?id=3159692">[paper]</a>
          </li>
          <li><i>Understanding and Predicting Delay in Reciprocal Relations</i><br>
            Jundong Li, Jiliang Tang, <strong>Yilin Wang</strong>, Yali Wan, Yi Chang, Huan Liu<br>
            WWW 2018 <span class="highlight">Research Track</span> <a href="https://arxiv.org/pdf/1703.01393.pdf">[arXiv]</a>
          </li>
          <li><i>Exploring Hierarchical Structures for Recommender Systems</i><br>
            Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            IEEE TKDE
          </li>
        </ul>
    
        <h4>2017</h4>
        <ul>
          <li><i>CLARE: A Joint Approach to Label Classification and Tag Recommendation</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Guojun Qi, Huan Liu, Baoxin Li<br>
            AAAI 2017 <span class="highlight">oral</span> <a href="papers/aaai_2017.pdf">[paper]</a> <a href="https://github.com/ywang370/ywang370.github.io/blob/master/admm_shoe.m">[code]</a>
          </li>
          <li><i>Understanding and Discovering Deliberate Self-harm Content in Social Media</i><br>
            <strong>Yilin Wang</strong>, Jiliang Tang, Jundong Li, Baoxin Li, Yali Wan, Clayton Mellina, Neil O'Hare, Yi Chang<br>
            WWW 2017 <span class="highlight">Research Track</span> <a href="papers/www_20171a.pdf">[paper]</a> <a href="papers/www2017.pdf">[slides]</a>
          </li>
          <li><i>Exploiting Hierarchical Structures for Unsupervised Feature Selection</i><br>
            Suhang Wang, <strong>Yilin Wang</strong>, Jiliang Tang, Charu Aggarwal, Suhas Ranganath, Huan Liu<br>
            SDM 2017 <a href="papers/sdm_2017.pdf">[paper]</a>
          </li>
          <li><i>What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest Recommendation</i><br>
            Suhang Wang, <strong>Yilin Wang</strong>, Jiliang Tang, Kai Shu, Suhas Ranganath, Huan Liu<br>
            WWW 2017 <span class="highlight">Research Track</span> <a href="papers/www_20171b.pdf">[paper]</a>
          </li>
        </ul>
    
        <h4>2016</h4>
        <ul>
          <li><i>PPP: Joint Pointwise and Pairwise Image Label Prediction</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Huan Liu, Baoxin Li<br>
            CVPR 2016 <a href="papers/PID4160349.pdf">[paper]</a>
          </li>
          <li><i>Efficient Unsupervised Abnormal Crowd Activity Detection Based on a Spatiotemporal Saliency Detector</i><br>
            <strong>Yilin Wang</strong>, Qiang Zhang, Baoxin Li<br>
            WACV 2016 <a href="papers/S3A_12.pdf">[paper]</a> <a href="https://github.com/ywang370/Video-Saliency">[code]</a>
          </li>
          <li><i>Scale Adaptive Eigen Eye for Fast Eye Detection in Wild Web Images</i><br>
            Xu Zhou, <strong>Yilin Wang</strong>, Peng Zhang, Baoxin Li<br>
            ICIP 2016
          </li>
        </ul>
    
        <h4>2015</h4>
        <ul>
          <li><i>Sentiment Analysis for Social Media Images</i><br>
            <strong>Yilin Wang</strong>, Baoxin Li<br>
            ICDM PhD Forum 2015
          </li>
          <li><i>Real Time Vehicle Back-up Warning System with Single Camera</i><br>
            <strong>Yilin Wang</strong>, Jun Cao, Baoxin Li<br>
            ICIP 2015 <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7351207&tag=1">[paper]</a>
          </li>
          <li><i>Unsupervised Sentiment Analysis for Social Media Images</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Huan Liu, Baoxin Li<br>
            IJCAI 2015 <a href="papers/Paper158_UESA.pdf">[paper]</a> <a href="projects/senti/senti.html">[project]</a>
          </li>
          <li><i>Inferring Sentiment from Web Images with Joint Inference on Visual and Social Cues: A Regulated Matrix Factorization Approach</i><br>
            <strong>Yilin Wang</strong>, Yuheng Hu, Subbarao Kambhampati, Baoxin Li<br>
            ICWSM 2015 <span class="highlight">oral</span> <a href="papers/latest_version_senti.pdf">[paper]</a>
          </li>
          <li><i>Structure Preserving Image Quality Assessment</i><br>
            <strong>Yilin Wang</strong>, Qiang Zhang, Baoxin Li<br>
            ICME 2015 <span class="highlight">oral</span> <a href="papers/271.pdf">[paper]</a>
          </li>
          <li><i>Exploring Implicit Hierarchical Structure for Recommender Systems</i><br>
            Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            IJCAI 2015 <a href="papers/paper140.pdf">[paper]</a>
          </li>
          <li><i>Improving Vision-based Self-positioning in Intelligent Transportation Systems via Integrated Lane and Vehicle Detection</i><br>
            Parag S. Chandakkar, <strong>Yilin Wang</strong>, Baoxin Li<br>
            WACV 2015 <a href="papers/midas.pdf">[paper]</a>
          </li>
        </ul>
    
        <h4>2014</h4>
        <ul>
          <li><i>Image Co-segmentation via Multi-task Learning</i><br>
            Qiang Zhang, Jiayu Zhou, <strong>Yilin Wang</strong>, Jieping Ye, Baoxin Li<br>
            BMVC 2014 <a href="papers/bmvc2014.pdf">[paper]</a>
          </li>
        </ul>
      </div>
    </div>
    

    </section>

    <!-- Service Section -->
    <section id="service">
      <h2><i class="bi bi-people-fill"></i> Service & Interns</h2>
      <ul>
        <li><strong>Area Chair:</strong> ACM MM 2020, 2021</li>
        <li><strong>Reviewer:</strong> CVPR, ICCV, ECCV, ICML, NeurIPS (since 2017)</li>
        <li><strong>Interns collaborated with:</strong> 
          <a href="https://zhke.io/">Zhanghan Ke</a>,
          <a href="https://cs-people.bu.edu/nnli/">Nannan Li</a>,
          <a href="https://yiqunmei.net/">Yiqun Mei</a>,
          <a href="https://yulunzhang.com/">Yulun Zhang</a>,
          <a href="https://shikun.io/">Shikun Liu</a>,
          <a href="https://www.chenglinyang.com/">Chenglin Yang</a>,
          <a href="https://jeya-maria-jose.github.io/research/">Jeya Maria Jose Valanarasu</a>,
          <a href="https://scholar.google.com.sg/citations?user=1SuLOuAAAAAJ&hl=en">Kenan E Ak</a>,
          <a href="https://yucornetto.github.io/">Qihang Yu</a>,
          <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Xin Yuan</a>,
          <a href="https://yifanjiang19.github.io/">Yifan Jiang</a>,
          <a href="https://g-jing.github.io/">Jing Gu</a>,
          <a href="https://scholar.google.com.hk/citations?user=Q88BI2QAAAAJ&hl=en">Zhibo Yang</a>
        </li>
      </ul>
    </section>

    <footer class="text-center">
      <p>&copy; 2025 Yilin Wang. Website modified from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.</p>
    </footer>
  </div>

  <!-- Back to Top Button -->
  <button class="back-to-top" aria-label="Back to top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">
    <i class="bi bi-arrow-up"></i>
  </button>

  <script>
    // Back to top button visibility
    const backToTop = document.querySelector('.back-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 300) {
        backToTop.classList.add('visible');
      } else {
        backToTop.classList.remove('visible');
      }
    });

    // Active nav link on scroll
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('.nav-link');
    
    window.addEventListener('scroll', () => {
      let current = '';
      sections.forEach(section => {
        const sectionTop = section.offsetTop - 100;
        if (scrollY >= sectionTop) {
          current = section.getAttribute('id');
        }
      });
      
      navLinks.forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href') === '#' + current) {
          link.classList.add('active');
        }
      });
    });
  </script>
</body>
</html>
