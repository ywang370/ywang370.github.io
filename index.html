<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Yilin Wang - Research Scientist at Adobe specializing in computer vision, image editing, and deep learning. Primary contributor to Photoshop's Select Subject, Object Finder, and Select People Details features.">
  <meta name="keywords" content="Yilin Wang, Adobe, Research Scientist, Computer Vision, Deep Learning, Image Editing, Photoshop, Machine Learning">
  <meta name="author" content="Yilin Wang">
  
  <!-- Open Graph / Social Media -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="Yilin Wang - Research Scientist at Adobe">
  <meta property="og:description" content="Research Scientist at Adobe specializing in computer vision, image editing, and deep learning.">
  <meta property="og:image" content="image/Wang-Yilin-8013b_meitu_1.jpg">
  <meta property="og:url" content="https://ywang370.github.io">
  <meta name="twitter:card" content="summary_large_image">
  
  <title>Yilin Wang - Research Scientist at Adobe</title>
  <link rel="icon" type="image/png" href="favicon.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.0/font/bootstrap-icons.css">
  <style>
    html { scroll-behavior: smooth; }
    body { font-family: 'Segoe UI', sans-serif; line-height: 1.6; background-color: #f9f9f9; color: #333; }
    .container { max-width: 1000px; margin: auto; padding: 2rem; }
    
    /* Navigation */
    .nav-pills .nav-link { color: #555; border-radius: 20px; padding: 0.4rem 1rem; margin: 0 0.2rem; transition: all 0.3s ease; }
    .nav-pills .nav-link:hover { background-color: #e9ecef; color: #333; }
    .nav-pills .nav-link.active { background-color: #0d6efd; color: white; }
    
    /* Profile */
    .profile-img { max-width: 200px; border-radius: 50%; margin-bottom: 1rem; box-shadow: 0 4px 15px rgba(0,0,0,0.1); transition: transform 0.3s ease; }
    .profile-img:hover { transform: scale(1.05); }
    
    /* Section headers */
    h2 { border-bottom: 2px solid #ddd; padding-bottom: 0.5rem; margin-top: 2rem; scroll-margin-top: 80px; }
    
    /* Contact links */
    .contact-links a { 
      margin-right: 0.5rem; 
      padding: 0.4rem 0.8rem; 
      border-radius: 20px; 
      background-color: #f0f0f0; 
      text-decoration: none; 
      color: #333; 
      transition: all 0.3s ease;
      display: inline-block;
      margin-bottom: 0.3rem;
    }
    .contact-links a:hover { background-color: #0d6efd; color: white; transform: translateY(-2px); }
    
    /* Paper entries */
    .red-star { color: red; }
    .highlighted-paper { background-color: #ffffd0; border-left: 5px solid #ffc107; }
    .paper-entry { 
      display: flex; 
      align-items: flex-start; 
      margin-bottom: 1.5rem; 
      background: #fff; 
      border-radius: 0.5rem; 
      padding: 1rem; 
      box-shadow: 0 2px 10px rgba(0,0,0,0.05); 
      transition: all 0.3s ease;
    }
    .paper-entry:hover { box-shadow: 0 5px 20px rgba(0,0,0,0.1); transform: translateY(-2px); }
    .paper-entry img { max-width: 180px; border-radius: 0.5rem; margin-right: 1.5rem; object-fit: cover; }
    .paper-details { flex: 1; }
    .paper-details h5 { margin-bottom: 0.5rem; }
    .paper-details h5 a { text-decoration: none; color: #0d6efd; }
    .paper-details h5 a:hover { text-decoration: underline; }
    .paper-details a { text-decoration: none; color: #0d6efd; }
    .paper-details a:hover { text-decoration: underline; }
    
    /* News list */
    .news-list { list-style: none; padding-left: 0; }
    .news-list li { padding: 0.5rem 0; border-bottom: 1px solid #eee; }
    .news-list li:last-child { border-bottom: none; }
    .news-date { color: #666; font-weight: 500; min-width: 100px; display: inline-block; }
    
    /* Back to top button */
    .back-to-top {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 45px;
      height: 45px;
      border-radius: 50%;
      background-color: #0d6efd;
      color: white;
      border: none;
      cursor: pointer;
      opacity: 0;
      visibility: hidden;
      transition: all 0.3s ease;
      z-index: 1000;
      box-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    .back-to-top.visible { opacity: 1; visibility: visible; }
    .back-to-top:hover { background-color: #0b5ed7; transform: translateY(-3px); }
    
    /* Responsive */
    @media (max-width: 768px) {
      .paper-entry { flex-direction: column; }
      .paper-entry img { max-width: 100%; margin-right: 0; margin-bottom: 1rem; }
      .nav-pills { justify-content: center; }
    }
    
    /* Hiring banner */
    .hiring-banner {
      background: linear-gradient(135deg, #ff6b6b, #ee5a5a);
      color: white;
      padding: 1rem;
      border-radius: 0.5rem;
      margin: 1rem 0;
      font-weight: 500;
    }
    
    /* Highlight badges for awards/oral */
    .highlight {
      background-color: #ffc107;
      color: #333;
      padding: 0.15rem 0.5rem;
      border-radius: 4px;
      font-size: 0.85em;
      font-weight: 500;
      margin-left: 0.3rem;
    }
    
    /* PhD Research section styling */
    .paper-year {
      display: flex;
      gap: 2rem;
      margin-bottom: 2rem;
    }
    .paper-year .institution-logo {
      flex-shrink: 0;
    }
    .paper-year h4 {
      color: #0d6efd;
      margin-top: 1.5rem;
      margin-bottom: 0.75rem;
      font-weight: 600;
    }
    .paper-year h4:first-child {
      margin-top: 0;
    }
    .paper-year ul {
      list-style: none;
      padding-left: 0;
    }
    .paper-year li {
      margin-bottom: 1rem;
      padding: 0.75rem;
      background: #fff;
      border-radius: 0.5rem;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    .paper-year li:hover {
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    @media (max-width: 768px) {
      .paper-year {
        flex-direction: column;
      }
      .paper-year .institution-logo {
        text-align: center;
      }
    }
  </style>
</head>
<body>
  <!-- Skip to main content for accessibility -->
  <a href="#main-content" class="visually-hidden-focusable">Skip to main content</a>
  
  <!-- Navigation -->
  <nav class="sticky-top bg-white py-2 shadow-sm mb-3">
    <div class="container">
      <ul class="nav nav-pills justify-content-center flex-wrap">
        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
        <li class="nav-item"><a class="nav-link" href="#news">News</a></li>
        <li class="nav-item"><a class="nav-link" href="#research">Research</a></li>
        <li class="nav-item"><a class="nav-link" href="#phd">PhD Work</a></li>
        <li class="nav-item"><a class="nav-link" href="#service">Service</a></li>
      </ul>
    </div>
  </nav>

  <div class="container" id="main-content">
    <!-- Profile Section -->
    <section id="about">
      <div class="text-center">
        <img src="image/Wang-Yilin-8013b_meitu_1.jpg" alt="Yilin Wang - Research Scientist at Adobe" class="profile-img" loading="lazy">
        <h1>Yilin Wang</h1>
        <p class="lead">Research Scientist at <a href="https://www.adobe.com/">Adobe</a>, San Jose</p>
        <div class="contact-links">
          <a href="mailto:wangyilin930@gmail.com"><i class="bi bi-envelope"></i> Email</a>
          <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ&hl=en"><i class="bi bi-mortarboard"></i> Scholar</a>
          <a href="https://www.linkedin.com/in/yilinasu/"><i class="bi bi-linkedin"></i> LinkedIn</a>
          <a href="https://github.com/ywang370"><i class="bi bi-github"></i> GitHub</a>
        </div>
      </div>

      <p class="mt-4">
        At Adobe, I work on computer vision for Imaging products. I am the primary contributor to several features including <a href="https://firefly.adobe.com">Instruction-based Image Editing</a>, <a href="https://theblog.adobe.com/photoshop-reimagined-for-ipad-introducing-select-subject/">Select Subject</a>, <a href="https://photoshopcafe.com/use-amazing-new-object-finder-photoshop-2022-instant-automatic-selections-objects-image/">Object Finder</a>, and <a href="https://fstoppers.com/photoshop/master-precise-adjustments-photoshops-latest-feature-688289">Select People Details</a>. I earned my Ph.D. at <a href="https://www.asu.edu/">Arizona State University</a>, advised by <a href="https://search.asu.edu/profile/747601">Baoxin Li</a>. I have contributed to 8 tech transfers to products including Photoshop, Lightroom, and Stardust.
      </p>

      <div class="hiring-banner">
        <i class="bi bi-megaphone"></i> Now recruiting summer research interns in image/video editing with MLLM. <a href="mailto:wangyilin930@gmail.com" class="text-white text-decoration-underline">Contact me!</a>
      </div>
    </section>

    <!-- News Section -->
    <section id="news">
      <h2><i class="bi bi-newspaper"></i> News</h2>
      <ul class="news-list">
        <li><span class="news-date">Feb. 2025</span> 3 papers accepted to CVPR 2025</li>
        <li><span class="news-date">Jan. 2025</span> 1 paper accepted to AAAI 2025</li>
        <li><span class="news-date">Nov. 2024</span> "Select People Details" featured by <a href="https://www.youtube.com/watch?v=8vO-9JT9MU0">Colin Smith on YouTube</a></li>
      </ul>
    </section>

    <!-- Research Section -->
    <section id="research">
      <h2><i class="bi bi-journal-richtext"></i> Highlighted Research</h2>
      <p><strong>I strive for simple yet scalable methods</strong> in image understanding and editing. Representative works are highlighted below. Full list available on <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ&hl=en">Google Scholar</a>.</p>
      
      <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
        <img src="images/oministyle.gif" alt="OmniStyle - Style transfer visualization">
        <div class="paper-details">
          <h5><a href="https://arxiv.org/abs/2505.14028">OmniStyle: Filtering High Quality Style Transfer Data at Scale</a></h5>
          <p class="mb-1">
            <a href="https://wangyephd.github.io/">Ye Wang</a>,
            Ruiqi Liu,
            Jiang Lin,
            <a href="https://is.nju.edu.cn/yzl_en/main.htm">Zili Yi</a>,
            <strong>Yilin Wang<span class="red-star">★</span></strong>,
            <a href="https://ruim-jlu.github.io/#about">Rui Ma<span class="red-star">★</span></a>
          </p>
          <p class="mb-1"><span class="red-star">★</span> co-advisor</p>
          <p class="mb-1">
            <a href="https://wangyephd.github.io/projects/cvpr25_omnistyle.html">project page</a> /
            <a href="https://arxiv.org/abs/2505.14028">paper</a>
          </p>
          <p><strong>CVPR 2025</strong></p>
          <p><strong>OmniStyle</strong> is the first end-to-end style transfer framework based on the Diffusion Transformer (DiT) architecture, achieving high-quality 1K-resolution stylization by leveraging the large-scale, filtered OmniStyle-1M dataset. It supports both instruction- and image-guided stylization, enabling efficient and versatile style transfer across diverse styles.</p>
        </div>
      </div>

    <div class="paper-entry">
      <img src="images/finecap.svg" alt="FineCaption - Compositional image captioning">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2312.14985">FINECAPTION: Compositional Image Captioning</a></h5>
        <p class="mb-1">
          Hang Hua,
          <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en">Qing Liu</a>,
          Lingzhi Zhang,
          Jing Shi,
          Zhifei Zhang,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          Jiebo Luo,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>
        </p>
        <p><strong>CVPR 2025</strong></p>
        <p>A unified vision-language model for free-form mask grounding and compositional captioning.</p>
      </div>
    </div>

    <div class="paper-entry">
      <img src="images/UniReal.gif" alt="UniReal - Universal image generation and editing">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2412.07774">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</a></h5>
        <p class="mb-1">
          Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, <strong>Yilin Wang</strong>, Hui Ding, Zhe Lin, Hengshuang Zhao
        </p>
        <p class="mb-1">
          <a href="https://arxiv.org/abs/2412.07774">pdf</a> /
          <a href="https://xavierchen34.github.io/UniReal-Page/">project page</a>
        </p>
        <p><strong>ICLR 2025 (Highlight)</strong></p>
        <p>
          <span style="color: red; font-weight: 500;">Foundational multi-modal generative model</span> — 
          UniReal is a universal framework for multiple image generation and editing tasks. We leverage a video model to handle image tasks by treating different numbers of input/output images as frames. We also seek universal supervisions from video data, thus generating realistic results that understand the world dynamics.
        </p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/sig_style1.jpg" alt="SigStyle - Signature style transfer">
      <div class="paper-details">
        <h5><a href="http://arxiv.org/abs/2404.05717">SigStyle: Signature Style Transfer via Personalized Text-to-Image Models</a></h5>
        <p class="mb-1">
          <a href="https://wangyephd.github.io/">Ye Wang</a>,
          Tongyuan Bai,
          Xuping Xie,
          <a href="https://is.nju.edu.cn/yzl_en/main.htm">Zili Yi</a>,
          <strong>Yilin Wang<span class="red-star">★</span></strong>,
          <a href="https://ruim-jlu.github.io/#about">Rui Ma<span class="red-star">★</span></a>
        </p>
        <p class="mb-1"><span class="red-star">★</span> co-advisor</p>
        <p class="mb-1">
          <a href="https://wangyephd.github.io/projects/sigstyle.html">project page</a> /
          <a href="http://arxiv.org/abs/2404.05717">paper</a>
        </p>
        <p><strong>AAAI 2025</strong></p>
        <p><strong>SigStyle</strong> is a style preserved style transfer method via personalized subject editing diffusion model.</p>
      </div>
    </div>

    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/swapanything_after.png" alt="SwapAnything" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="http://arxiv.org/abs/2404.05717">SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing</a></h5>
        <p class="mb-1"><strong>ECCV 2024</strong></p>
        <p class="mb-1">
          <a href="https://g-jing.github.io/">Jing Gu</a>,
          <a href="http://nxzhao.com//">Nanxuan Zhao</a>,
          <a href="https://wxiong.me/">Wei Xiong</a>,
          <a href="https://qliu24.github.io/">Qing Liu</a>,
          <a href="https://zzutk.github.io/">Zhifei Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>,
          <strong>Yilin Wang</strong><span class="red-star">★</span>,
          <a href="https://eric-xw.github.io/">Xin Eric Wang</a><span class="red-star">★</span>
        </p>
        <p class="mb-1"><span class="red-star">★</span> Co-advisor</p>
        <p class="mb-1"><a href="https://swap-anything.github.io/">project page</a> / <a href="http://arxiv.org/abs/2404.05717">paper</a></p>
        <p>A method for personalized subject driven image editing.</p>
      </div>
    </div>

    <div class="paper-entry">
      <img src="images/unihuman.png" alt="UniHuman" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2312.14985">UniHuman: A Unified Model for Editing Human Images in the Wild.</a></h5>
        <p class="mb-1"><strong>CVPR 2024</strong></p>
        <p class="mb-1">
          Nannan Li,
          <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en">Qing Liu</a>,
          <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          Bryan A. Plummer,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>
        </p>
        <p>Human editing via diffusion.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/amodal.png" alt="Amodal" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5>Amodal Scene Analysis via Holistic Occlusion Relation Inference and Generative Mask Completion</h5>
        <p class="mb-1"><strong>AAAI (oral) 2024</strong></p>
        <p class="mb-1">
          Bowen Zhang,
          <a href="https://sites.google.com/site/hezhangsprinter/">Qing Liu</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          Akide Liu,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <a href="https://scholar.google.com/citations?user=ksQ4JnQAAAAJ&hl=zh-CN">Yifan Liu</a>
        </p>
        <p><a href="#">project page</a> / <a href="#">paper</a></p>
        <p>Amodal segmentation considers mutual occlusion.</p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/photoswap_after.png" alt="PhotoSwap" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2305.18286">PHOTOSWAP: Personalized Subject Swapping in Images</a></h5>
        <p class="mb-1"><strong>NeurIPS 2023</strong></p>
        <p class="mb-1">
          <a href="https://g-jing.github.io/">Jing Gu</a>,
          <strong>Yilin Wang</strong>,
          <a href="http://nxzhao.com//">Nanxuan Zhao</a>,
          <a href="https://tsujuifu.github.io/">Tsu-Jui Fu</a>,
          <a href="https://wxiong.me/">Wei Xiong</a>,
          <a href="https://qliu24.github.io/">Qing Liu</a>,
          <a href="https://zzutk.github.io/">Zhifei Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>,
          <a href="https://eric-xw.github.io/">Xin Eric Wang</a>
        </p>
        <p><a href="https://photoswap.github.io/">project page</a> / <a href="https://arxiv.org/pdf/2305.18286.pdf">paper</a></p>
        <p>A method for personalized subject driven image editing.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/lightpainter.png" alt="LightPainter" style="width:160px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2303.12950">LightPainter: Interactive Portrait Relighting with Freehand Scribble</a></h5>
        <p><strong>CVPR 2023</strong></p>
        <p>
          <a href="https://yiqunmei.net/">Yiqun Mei</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://zhixinshu.github.io/">Zhixin Shu</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>,
          Yan Shi,
          <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>,
          <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en">Vishal M. Patel</a>
        </p>
        <p><a href="https://yiqunmei.net/lightpt/">project page</a> / <a href="https://yiqunmei.net/lightpt/resources/CVPR2023_LightPainter.pdf">paper</a></p>
        <p>A scribble-based relighting system that allows users to interactively manipulate portrait lighting effects with ease.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/interactive.png" alt="Interactive Portrait Harmonization" style="width:160px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2203.08216">Interactive Portrait Harmonization</a></h5>
        <p><strong>ICLR 2023</strong></p>
        <p>
          <a href="https://jeya-maria-jose.github.io/research/">Jeya Maria Jose Valanarasu</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="https://www.linkedin.com/in/yinglan-ma/">Yinglan Ma</a>,
          <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>,
          <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
          <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en">Vishal M. Patel</a>
        </p>
        <p><a href="https://jeya-maria-jose.github.io/IPH-web/">project page</a> / <a href="https://arxiv.org/abs/2203.08216">paper</a></p>
        <p>Interactive harmonization for portrait photos.</p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/lvt.png" alt="Lite Vision Transformer" style="width:190px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2112.10809">Lite Vision Transformer with Enhanced Self-Attention</a></h5>
        <p><strong>CVPR 2022</strong></p>
        <p>
          <a href="https://www.chenglinyang.com/">Chenglin Yang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
        </p>
        <p><a href="https://www.chenglinyang.com/publication/lvt/">project page</a> / <a href="https://arxiv.org/abs/2112.10809">paper</a></p>
        <p>Light-weight vision transformer models for vision tasks.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/ssh.png" alt="SSH" style="width:190px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2108.06805">SSH: A Self-Supervised Framework for Image Harmonization</a></h5>
        <p><strong>ICCV 2021</strong></p>
        <p>
          <a href="https://www.chenglinyang.com/">Yifan Jiang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
          Simon Chen,
          <a href="https://scholar.google.com/citations?user=aFrtZOIAAAAJ&hl=en">Sohrab Amirghodsi</a>,
          <a href="https://blog.adobe.com/en/publish/2017/08/02/women-in-technology-sarah-kong">Sarah Kong</a>,
          <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Zhangyang Wang</a>
        </p>
        <p><a href="https://arxiv.org/abs/2108.06805">paper</a></p>
        <p>Image harmonization based on self-supervised learning.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/yu2020mask.png" alt="Mask Guided Matting" style="width:190px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/pdf/2012.06722.pdf">Mask Guided Matting via Progressive Refinement Network</a></h5>
        <p><strong>CVPR 2021</strong></p>
        <p>
          <a href="https://yucornetto.github.io/">Qihang Yu</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
          <a href="https://yutongbai.com/">Yutong Bai</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
        </p>
        <p><a href="https://github.com/yucornetto/MGMatting">project page</a> / <a href="https://arxiv.org/pdf/2012.06722.pdf">paper</a></p>
        <p>Mask guided image matting.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/mct.png" alt="Multimodal Contrastive Training" style="width:160px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf">Multimodal Contrastive Training for Visual Representation Learning</a></h5>
        <p><strong>CVPR 2021</strong></p>
        <p>
          <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Xin Yuan</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <a href="https://scholar.google.com.my/citations?user=e6u7GlQAAAAJ&hl=en">Jason Kuen</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          Michael Maire,
          Ajinkya Kale,
          Baldo Faieta
        </p>
        <p><a href="#">project page</a> / <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf">paper</a></p>
        <p>Intra- and inter-modal similarity preservation for multimodal representation learning.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/RAL.png" alt="RAL" style="width:160px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/pdf/2007.09923">Incorporating Reinforced Adversarial Learning in Autoregressive Image Generation</a></h5>
        <p><strong>ECCV 2020</strong></p>
        <p>
          <a href="https://scholar.google.com.sg/citations?user=1SuLOuAAAAAJ&hl=en">Kenan E. Ak</a>,
          <a href="https://sites.google.com/view/ningxu/homepage">Ning Xu</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <strong>Yilin Wang</strong>
        </p>
        <p><a href="https://arxiv.org/pdf/2007.09923">paper</a></p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/shape.png" alt="Shape Adaptor" style="width:160px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/pdf/2007.09923">Shape Adaptor: A Learnable Resizing Module</a></h5>
        <p><strong>ECCV 2020</strong></p>
        <p>
          <a href="https://shikun.io/">Shikun Liu</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://fperazzi.github.io/">Federico Perazzi</a>,
          <a href="https://scholar.google.co.uk/citations?user=dHec-LkAAAAJ&hl=en">Edward Johns</a>
        </p>
        <p><a href="https://arxiv.org/pdf/2007.09923">paper</a></p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/mmc.png" alt="MMC" style="width:160px; height:190px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/1904.04443">Multimodal Style Transfer via Graph Cuts</a></h5>
        <p><strong>ICCV 2019</strong></p>
        <p>
          <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Yulun Zhang</a>,
          <a href="https://scholar.google.com.my/citations?user=e6u7GlQAAAAJ&hl=en">Chen Fang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://sites.google.com/site/zhelin625/">Zhaowen Wang</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <a href="https://www1.ece.neu.edu/~yunfu/">Yun Fu</a>,
          <a href="https://jimeiyang.github.io/">Jimei Yang</a>
        </p>
        <p><a href="https://arxiv.org/abs/1904.04443">paper</a></p>
      </div>
    </div>
    

    </section>

    <!-- PhD Research Section -->
    <section id="phd">
      <h2><i class="bi bi-mortarboard-fill"></i> PhD Research</h2>
    
    <div class="paper-year">
      <div class="institution-logo">
        <img src="images/asu.avif" alt="ASU" style="width:100%; max-width:200px; border-radius:8px;">
      </div>
      <div class="paper-details">
        <h4>2018</h4>
        <ul>
          <li><i>Generalizing Graph Matching beyond Quadratic Assignment Model</i><br>
            Tianshu Yu, Junchi Yan, <strong>Yilin Wang</strong>, Wei Liu, Baoxin Li<br>
            NeurIPS 2018
          </li>
          <li><i>Weakly Supervised Facial Attribute Manipulation via Deep Adversarial Network</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Guojun Qi, Jiliang Tang, Baoxin Li<br>
            WACV 2018 <a href="papers/wacv18.pdf">[paper]</a>
          </li>
          <li><i>CrossFire: Cross Media Joint Friend and Item Recommendations</i><br>
            Kai Shu, Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            WSDM 2018 <span class="highlight">spotlight</span> <a href="https://dl.acm.org/citation.cfm?id=3159692">[paper]</a>
          </li>
          <li><i>Understanding and Predicting Delay in Reciprocal Relations</i><br>
            Jundong Li, Jiliang Tang, <strong>Yilin Wang</strong>, Yali Wan, Yi Chang, Huan Liu<br>
            WWW 2018 <span class="highlight">Research Track</span> <a href="https://arxiv.org/pdf/1703.01393.pdf">[arXiv]</a>
          </li>
          <li><i>Exploring Hierarchical Structures for Recommender Systems</i><br>
            Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            IEEE TKDE
          </li>
        </ul>
    
        <h4>2017</h4>
        <ul>
          <li><i>CLARE: A Joint Approach to Label Classification and Tag Recommendation</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Guojun Qi, Huan Liu, Baoxin Li<br>
            AAAI 2017 <span class="highlight">oral</span> <a href="papers/aaai_2017.pdf">[paper]</a> <a href="https://github.com/ywang370/ywang370.github.io/blob/master/admm_shoe.m">[code]</a>
          </li>
          <li><i>Understanding and Discovering Deliberate Self-harm Content in Social Media</i><br>
            <strong>Yilin Wang</strong>, Jiliang Tang, Jundong Li, Baoxin Li, Yali Wan, Clayton Mellina, Neil O'Hare, Yi Chang<br>
            WWW 2017 <span class="highlight">Research Track</span> <a href="papers/www_20171a.pdf">[paper]</a> <a href="papers/www2017.pdf">[slides]</a>
          </li>
          <li><i>Exploiting Hierarchical Structures for Unsupervised Feature Selection</i><br>
            Suhang Wang, <strong>Yilin Wang</strong>, Jiliang Tang, Charu Aggarwal, Suhas Ranganath, Huan Liu<br>
            SDM 2017 <a href="papers/sdm_2017.pdf">[paper]</a>
          </li>
          <li><i>What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest Recommendation</i><br>
            Suhang Wang, <strong>Yilin Wang</strong>, Jiliang Tang, Kai Shu, Suhas Ranganath, Huan Liu<br>
            WWW 2017 <span class="highlight">Research Track</span> <a href="papers/www_20171b.pdf">[paper]</a>
          </li>
        </ul>
    
        <h4>2016</h4>
        <ul>
          <li><i>PPP: Joint Pointwise and Pairwise Image Label Prediction</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Huan Liu, Baoxin Li<br>
            CVPR 2016 <a href="papers/PID4160349.pdf">[paper]</a>
          </li>
          <li><i>Efficient Unsupervised Abnormal Crowd Activity Detection Based on a Spatiotemporal Saliency Detector</i><br>
            <strong>Yilin Wang</strong>, Qiang Zhang, Baoxin Li<br>
            WACV 2016 <a href="papers/S3A_12.pdf">[paper]</a> <a href="https://github.com/ywang370/Video-Saliency">[code]</a>
          </li>
          <li><i>Scale Adaptive Eigen Eye for Fast Eye Detection in Wild Web Images</i><br>
            Xu Zhou, <strong>Yilin Wang</strong>, Peng Zhang, Baoxin Li<br>
            ICIP 2016
          </li>
        </ul>
    
        <h4>2015</h4>
        <ul>
          <li><i>Sentiment Analysis for Social Media Images</i><br>
            <strong>Yilin Wang</strong>, Baoxin Li<br>
            ICDM PhD Forum 2015
          </li>
          <li><i>Real Time Vehicle Back-up Warning System with Single Camera</i><br>
            <strong>Yilin Wang</strong>, Jun Cao, Baoxin Li<br>
            ICIP 2015 <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7351207&tag=1">[paper]</a>
          </li>
          <li><i>Unsupervised Sentiment Analysis for Social Media Images</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Huan Liu, Baoxin Li<br>
            IJCAI 2015 <a href="papers/Paper158_UESA.pdf">[paper]</a> <a href="projects/senti/senti.html">[project]</a>
          </li>
          <li><i>Inferring Sentiment from Web Images with Joint Inference on Visual and Social Cues: A Regulated Matrix Factorization Approach</i><br>
            <strong>Yilin Wang</strong>, Yuheng Hu, Subbarao Kambhampati, Baoxin Li<br>
            ICWSM 2015 <span class="highlight">oral</span> <a href="papers/latest_version_senti.pdf">[paper]</a>
          </li>
          <li><i>Structure Preserving Image Quality Assessment</i><br>
            <strong>Yilin Wang</strong>, Qiang Zhang, Baoxin Li<br>
            ICME 2015 <span class="highlight">oral</span> <a href="papers/271.pdf">[paper]</a>
          </li>
          <li><i>Exploring Implicit Hierarchical Structure for Recommender Systems</i><br>
            Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            IJCAI 2015 <a href="papers/paper140.pdf">[paper]</a>
          </li>
          <li><i>Improving Vision-based Self-positioning in Intelligent Transportation Systems via Integrated Lane and Vehicle Detection</i><br>
            Parag S. Chandakkar, <strong>Yilin Wang</strong>, Baoxin Li<br>
            WACV 2015 <a href="papers/midas.pdf">[paper]</a>
          </li>
        </ul>
    
        <h4>2014</h4>
        <ul>
          <li><i>Image Co-segmentation via Multi-task Learning</i><br>
            Qiang Zhang, Jiayu Zhou, <strong>Yilin Wang</strong>, Jieping Ye, Baoxin Li<br>
            BMVC 2014 <a href="papers/bmvc2014.pdf">[paper]</a>
          </li>
        </ul>
      </div>
    </div>
    

    </section>

    <!-- Service Section -->
    <section id="service">
      <h2><i class="bi bi-people-fill"></i> Service & Interns</h2>
      <ul>
        <li><strong>Area Chair:</strong> ACM MM 2020, 2021</li>
        <li><strong>Reviewer:</strong> CVPR, ICCV, ECCV, ICML, NeurIPS (since 2017)</li>
        <li><strong>Interns collaborated with:</strong> 
          <a href="https://zhke.io/">Zhanghan Ke</a>,
          <a href="https://cs-people.bu.edu/nnli/">Nannan Li</a>,
          <a href="https://yiqunmei.net/">Yiqun Mei</a>,
          <a href="https://yulunzhang.com/">Yulun Zhang</a>,
          <a href="https://shikun.io/">Shikun Liu</a>,
          <a href="https://www.chenglinyang.com/">Chenglin Yang</a>,
          <a href="https://jeya-maria-jose.github.io/research/">Jeya Maria Jose Valanarasu</a>,
          <a href="https://scholar.google.com.sg/citations?user=1SuLOuAAAAAJ&hl=en">Kenan E Ak</a>,
          <a href="https://yucornetto.github.io/">Qihang Yu</a>,
          <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Xin Yuan</a>,
          <a href="https://yifanjiang19.github.io/">Yifan Jiang</a>,
          <a href="https://g-jing.github.io/">Jing Gu</a>,
          <a href="https://scholar.google.com.hk/citations?user=Q88BI2QAAAAJ&hl=en">Zhibo Yang</a>
        </li>
      </ul>
    </section>

    <footer class="text-center mt-5 py-4 border-top">
      <p class="text-muted" style="font-size:small;">
        &copy; 2025 Yilin Wang. Website modified from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
      </p>
    </footer>
  </div>

  <!-- Back to Top Button -->
  <button class="back-to-top" aria-label="Back to top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">
    <i class="bi bi-arrow-up"></i>
  </button>

  <script>
    // Back to top button visibility
    const backToTop = document.querySelector('.back-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 300) {
        backToTop.classList.add('visible');
      } else {
        backToTop.classList.remove('visible');
      }
    });

    // Active nav link on scroll
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('.nav-link');
    
    window.addEventListener('scroll', () => {
      let current = '';
      sections.forEach(section => {
        const sectionTop = section.offsetTop - 100;
        if (scrollY >= sectionTop) {
          current = section.getAttribute('id');
        }
      });
      
      navLinks.forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href') === '#' + current) {
          link.classList.add('active');
        }
      });
    });
  </script>
</body>
</html>
