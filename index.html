<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Yilin Wang</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
  <style>
    body { font-family: 'Segoe UI', sans-serif; line-height: 1.6; background-color: #f9f9f9; color: #333; }
    .container { max-width: 1000px; margin: auto; padding: 2rem; }
    .profile-img { max-width: 200px; border-radius: 50%; margin-bottom: 1rem; }
    .highlighted-paper { background-color: #fff3cd; border-left: 5px solid #ffc107; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem; }
    h2 { border-bottom: 2px solid #ddd; padding-bottom: 0.5rem; margin-top: 2rem; }
    .contact-links a { margin-right: 1rem; }
    .red-star { color: red; }
    .paper-entry { display: flex; align-items: center; margin-bottom: 1.5rem; background: #fff; border-radius: 0.5rem; padding: 1rem; box-shadow: 0 0 10px rgba(0,0,0,0.05); }
    .paper-entry img { max-width: 180px; border-radius: 0.5rem; margin-right: 1.5rem; }
    .paper-details a { text-decoration: none; color: #0d6efd; }
    .paper-details a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <div class="container">
    <div class="text-center">
      <img src="image/Wang-Yilin-8013b_meitu_1.jpg" alt="Yilin Wang" class="profile-img">
      <h1>Yilin Wang</h1>
      <p>Research Scientist at <a href="https://www.adobe.com/">Adobe</a>, San Jose</p>
      <div class="contact-links">
        <a href="mailto:wangyilin930@gmail.com">Email</a>
        <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ&hl=en">Scholar</a>
        <a href="https://www.linkedin.com/in/yilinasu/">LinkedIn</a>
      </div>
    </div>

    <p class="mt-4">
      At Adobe, I work on computer vision for Imaging products. I am the primary contributor to several features including <a href="https://firefly.adobe.com">Instruction-based Image Editing</a>, <a href="https://theblog.adobe.com/photoshop-reimagined-for-ipad-introducing-select-subject/">Select Subject</a>, <a href="https://photoshopcafe.com/use-amazing-new-object-finder-photoshop-2022-instant-automatic-selections-objects-image/">Object Finder</a>, and <a href="https://fstoppers.com/photoshop/master-precise-adjustments-photoshops-latest-feature-688289">Select People Details</a>. I earned my Ph.D. at <a href="https://www.asu.edu/">Arizona State University</a>, advised by <a href="https://search.asu.edu/profile/747601">Baoxin Li</a>. I have contributed to 8 tech transfers to products including Photoshop, Lightroom, and Stardust.
    </p>

    <p class="text-danger fw-bold">Now recruiting summer research interns in image/video editing with MLLM.</p>

    <h2>News</h2>
    <ul>
      <li>Feb. 2025 – 3 papers accepted to CVPR 2025</li>
      <li>Jan. 2025 – 1 paper accepted to AAAI 2025</li>
      <li>Nov. 2024 – "Select People Details" featured by <a href="https://www.youtube.com/watch?v=8vO-9JT9MU0">Colin Smith on YouTube</a></li>
    </ul>

    <h2>Highlighted Research</h2>
    <p><strong>I strive for simple yet scalable methods</strong> in image understanding and editing. Representative works are highlighted below. Full list available on <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ&hl=en">Google Scholar</a>.</p>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/oministyle.gif" alt="OmniStyle">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2505.14028">OmniStyle: Filtering High Quality Style Transfer Data at Scale</a></h5>
        <a href="https://wangyephd.github.io/">Ye Wang</a>,
        <a>Ruiqi Liu</a>,
        <a >Jiang Lin</a>,
        <a href="https://is.nju.edu.cn/yzl_en/main.htm">Zili Yi</a>,
          <strong>Yilin Wang<span class="red-star">★</span></strong>
        <a href="https://ruim-jlu.github.io/#about">Rui Ma<span class="red-star">★</span></a>
        <br>
          <span class="red-star">★</span> co-advisor </br>
        <a href="https://wangyephd.github.io/projects/cvpr25_omnistyle.html">project page</a> /
        <a href="https://arxiv.org/abs/2505.14028">paper</a> /

        <p><strong>CVPR 2025</strong><br>
          <strong>OmniStyle</strong> is the first end-to-end style transfer framework based on the Diffusion Transformer (DiT) architecture, achieving high-quality 1K-resolution stylization by leveraging the large-scale, filtered OmniStyle-1M dataset. It supports both instruction- and image-guided stylization, enabling efficient and versatile style transfer across diverse styles.
      </div>
    </div>

    <div class="paper-entry">
      <img src="images/finecap.svg" alt="FineCaption">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2312.14985">FINECAPTION: Compositional Image Captioning</a></h5>
        <a href=>Hang Hua</a>,
      <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en">Qing Liu</a>,
      <a>Lingzhi Zhang</a>,
      <a>Jing Shi</a>,
      <a>Zhifei Zhang</a>,
      <strong>Yilin Wang</strong>,
      <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
      <a >Jiebo Luo</a>,
      <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
      <p><strong>CVPR 2025</strong><br>
        A unified vision-language model for free-form mask grounding and compositional captioning.</p>
      </div>
    </div>

    <div class="paper-entry">
      <img src="images/UniReal.gif" alt="UniReal">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2412.07774">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</a></h5>
        Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, <strong>Yilin Wang</strong>, Hui Ding, Zhe Lin, Hengshuang Zhao <br>
        <p><strong> 2025 (Highlight)</strong><br>
        <a href="https://arxiv.org/abs/2412.07774">pdf</a>/
        <a href="https://xavierchen34.github.io/UniReal-Page/">project page</a>
        <p></p>
        <p>
        <span style="color: red;">Foundaitional multi-modal generative model</span>  
          UniReal is a universal framework for multiple image generation and editing tasks. We leverage a video model to handld image tasks by treating different numbers of 
          input/output images as frames. We also seek universal supervisions from video data, thus generating realistic results that understand the world dynamics.
        </p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/sig_style1.jpg" alt="SigStyle">
      <div class="paper-details">
        <h5><a href="http://arxiv.org/abs/2404.05717">SigStyle: Signature Style Transfer via Personalized Text-to-Image Models</a></h5>
        <a href="https://wangyephd.github.io/">Ye Wang</a>,
          <a>Tongyuan Bai</a>,
          <a >Xuping Xie</a>,
          <a href="https://is.nju.edu.cn/yzl_en/main.htm">Zili Yi</a>,
            <strong>Yilin Wang<span class="red-star">★</span></strong>
          <a href="https://ruim-jlu.github.io/#about">Rui Ma<span class="red-star">★</span></a>
          <br>
            <span class="red-star">★</span> co-advisor </br>
          <a href="https://wangyephd.github.io/projects/sigstyle.html">project page</a> /
          <a href="http://arxiv.org/abs/2404.05717">paper</a> /

        <p><strong>AAAI 2025</strong><br>
          <strong>Sigstyle</strong> is a style preserved style transfer method via personalized subject editing diffusion model.
      </div>
    </div>

    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/swapanything_after.png" alt="SwapAnything" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="http://arxiv.org/abs/2404.05717">SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing</a></h5>
        <p class="mb-1"><strong>ECCV 2024</strong></p>
        <p class="mb-1">
          <a href="https://g-jing.github.io/">Jing Gu</a>,
          <a href="http://nxzhao.com//">Nanxuan Zhao</a>,
          <a href="https://wxiong.me/">Wei Xiong</a>,
          <a href="https://qliu24.github.io/">Qing Liu</a>,
          <a href="https://zzutk.github.io/">Zhifei Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>,
          <strong>Yilin Wang</strong><span class="red-star">★</span>,
          <a href="https://eric-xw.github.io/">Xin Eric Wang</a><span class="red-star">★</span>
        </p>
        <p class="mb-1"><span class="red-star">★</span> Co-advisor</p>
        <p class="mb-1"><a href="https://swap-anything.github.io/">project page</a> / <a href="http://arxiv.org/abs/2404.05717">paper</a></p>
        <p>A method for personalized subject driven image editing.</p>
      </div>
    </div>

    <div class="paper-entry">
      <img src="images/unihuman.png" alt="UniHuman" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2312.14985">UniHuman: A Unified Model for Editing Human Images in the Wild.</a></h5>
        <p class="mb-1"><strong>CVPR 2024</strong></p>
        <p class="mb-1">
          Nannan Li,
          <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en">Qing Liu</a>,
          <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          Bryan A. Plummer,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>
        </p>
        <p>Human editing via diffusion.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/amodal.png" alt="Amodal" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5>Amodal Scene Analysis via Holistic Occlusion Relation Inference and Generative Mask Completion</h5>
        <p class="mb-1"><strong>AAAI (oral) 2024</strong></p>
        <p class="mb-1">
          Bowen Zhang,
          <a href="https://sites.google.com/site/hezhangsprinter/">Qing Liu</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          Akide Liu,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <a href="https://scholar.google.com/citations?user=ksQ4JnQAAAAJ&hl=zh-CN">Yifan Liu</a>
        </p>
        <p><a href="#">project page</a> / <a href="#">paper</a></p>
        <p>Amodal segmentation considers mutual occlusion.</p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/photoswap_after.png" alt="PhotoSwap" style="width:180px; height:130px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2305.18286">PHOTOSWAP: Personalized Subject Swapping in Images</a></h5>
        <p class="mb-1"><strong>NeurIPS 2023</strong></p>
        <p class="mb-1">
          <a href="https://g-jing.github.io/">Jing Gu</a>,
          <strong>Yilin Wang</strong>,
          <a href="http://nxzhao.com//">Nanxuan Zhao</a>,
          <a href="https://tsujuifu.github.io/">Tsu-Jui Fu</a>,
          <a href="https://wxiong.me/">Wei Xiong</a>,
          <a href="https://qliu24.github.io/">Qing Liu</a>,
          <a href="https://zzutk.github.io/">Zhifei Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>,
          <a href="https://eric-xw.github.io/">Xin Eric Wang</a>
        </p>
        <p><a href="https://photoswap.github.io/">project page</a> / <a href="https://arxiv.org/pdf/2305.18286.pdf">paper</a></p>
        <p>A method for personalized subject driven image editing.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/lightpainter.png" alt="LightPainter" style="width:160px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2303.12950">LightPainter: Interactive Portrait Relighting with Freehand Scribble</a></h5>
        <p><strong>CVPR 2023</strong></p>
        <p>
          <a href="https://yiqunmei.net/">Yiqun Mei</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://zhixinshu.github.io/">Zhixin Shu</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>,
          Yan Shi,
          <a href="https://polaris79.wixsite.com/hjung">HyunJoon Jung</a>,
          <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en">Vishal M. Patel</a>
        </p>
        <p><a href="https://yiqunmei.net/lightpt/">project page</a> / <a href="https://yiqunmei.net/lightpt/resources/CVPR2023_LightPainter.pdf">paper</a></p>
        <p>A scribble-based relighting system that allows users to interactively manipulate portrait lighting effects with ease.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/interactive.png" alt="Interactive Portrait Harmonization" style="width:160px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2203.08216">Interactive Portrait Harmonization</a></h5>
        <p><strong>ICLR 2023</strong></p>
        <p>
          <a href="https://jeya-maria-jose.github.io/research/">Jeya Maria Jose Valanarasu</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="https://www.linkedin.com/in/yinglan-ma/">Yinglan Ma</a>,
          <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>,
          <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
          <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en">Vishal M. Patel</a>
        </p>
        <p><a href="https://jeya-maria-jose.github.io/IPH-web/">project page</a> / <a href="https://arxiv.org/abs/2203.08216">paper</a></p>
        <p>Interactive harmonization for portrait photos.</p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/lvt.png" alt="Lite Vision Transformer" style="width:190px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2112.10809">Lite Vision Transformer with Enhanced Self-Attention</a></h5>
        <p><strong>CVPR 2022</strong></p>
        <p>
          <a href="https://www.chenglinyang.com/">Chenglin Yang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://scholar.google.com/citations?user=8l3bFYYAAAAJ&hl=en">Zijun Wei</a>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
        </p>
        <p><a href="https://www.chenglinyang.com/publication/lvt/">project page</a> / <a href="https://arxiv.org/abs/2112.10809">paper</a></p>
        <p>Light-weight vision transformer models for vision tasks.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/ssh.png" alt="SSH" style="width:190px; height:160px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/2108.06805">SSH: A Self-Supervised Framework for Image Harmonization</a></h5>
        <p><strong>ICCV 2021</strong></p>
        <p>
          <a href="https://www.chenglinyang.com/">Yifan Jiang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>,
          Simon Chen,
          <a href="https://scholar.google.com/citations?user=aFrtZOIAAAAJ&hl=en">Sohrab Amirghodsi</a>,
          <a href="https://blog.adobe.com/en/publish/2017/08/02/women-in-technology-sarah-kong">Sarah Kong</a>,
          <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Zhangyang Wang</a>
        </p>
        <p><a href="https://arxiv.org/abs/2108.06805">paper</a></p>
        <p>Image harmonization based on self-supervised learning.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/yu2020mask.png" alt="Mask Guided Matting" style="width:190px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/pdf/2012.06722.pdf">Mask Guided Matting via Progressive Refinement Network</a></h5>
        <p><strong>CVPR 2021</strong></p>
        <p>
          <a href="https://yucornetto.github.io/">Qihang Yu</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://sites.google.com/site/hezhangsprinter/">He Zhang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
          <a href="https://sites.google.com/view/ningxu/">Ning Xu</a>,
          <a href="https://yutongbai.com/">Yutong Bai</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
        </p>
        <p><a href="https://github.com/yucornetto/MGMatting">project page</a> / <a href="https://arxiv.org/pdf/2012.06722.pdf">paper</a></p>
        <p>Mask guided image matting.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/mct.png" alt="Multimodal Contrastive Training" style="width:160px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf">Multimodal Contrastive Training for Visual Representation Learning</a></h5>
        <p><strong>CVPR 2021</strong></p>
        <p>
          <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Xin Yuan</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <a href="https://scholar.google.com.my/citations?user=e6u7GlQAAAAJ&hl=en">Jason Kuen</a>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <strong>Yilin Wang</strong>,
          Michael Maire,
          Ajinkya Kale,
          Baldo Faieta
        </p>
        <p><a href="#">project page</a> / <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf">paper</a></p>
        <p>Intra- and inter-modal similarity preservation for multimodal representation learning.</p>
      </div>
    </div>
    
    <div class="paper-entry">
      <img src="images/RAL.png" alt="RAL" style="width:160px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/pdf/2007.09923">Incorporating Reinforced Adversarial Learning in Autoregressive Image Generation</a></h5>
        <p><strong>ECCV 2020</strong></p>
        <p>
          <a href="https://scholar.google.com.sg/citations?user=1SuLOuAAAAAJ&hl=en">Kenan E. Ak</a>,
          <a href="https://sites.google.com/view/ningxu/homepage">Ning Xu</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <strong>Yilin Wang</strong>
        </p>
        <p><a href="https://arxiv.org/pdf/2007.09923">paper</a></p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/shape.png" alt="Shape Adaptor" style="width:160px; height:100px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/pdf/2007.09923">Shape Adaptor: A Learnable Resizing Module</a></h5>
        <p><strong>ECCV 2020</strong></p>
        <p>
          <a href="https://shikun.io/">Shikun Liu</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://cs-people.bu.edu/jmzhang/">Jianming Zhang</a>,
          <a href="https://fperazzi.github.io/">Federico Perazzi</a>,
          <a href="https://scholar.google.co.uk/citations?user=dHec-LkAAAAJ&hl=en">Edward Johns</a>
        </p>
        <p><a href="https://arxiv.org/pdf/2007.09923">paper</a></p>
      </div>
    </div>
    
    <div class="paper-entry highlighted-paper" style="background-color:#ffffd0;">
      <img src="images/mmc.png" alt="MMC" style="width:160px; height:190px; margin-right:20px; border-radius:8px;">
      <div class="paper-details">
        <h5><a href="https://arxiv.org/abs/1904.04443">Multimodal Style Transfer via Graph Cuts</a></h5>
        <p><strong>ICCV 2019</strong></p>
        <p>
          <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Yulun Zhang</a>,
          <a href="https://scholar.google.com.my/citations?user=e6u7GlQAAAAJ&hl=en">Chen Fang</a>,
          <strong>Yilin Wang</strong>,
          <a href="https://sites.google.com/site/zhelin625/">Zhaowen Wang</a>,
          <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
          <a href="https://www1.ece.neu.edu/~yunfu/">Yun Fu</a>,
          <a href="https://jimeiyang.github.io/">Jimei Yang</a>
        </p>
        <p><a href="https://arxiv.org/abs/1904.04443">paper</a></p>
      </div>
    </div>
    

    <div class="section">
      <h2>PhD Research</h2>
    </div>
    
    <div class="paper-year">
      <div class="institution-logo">
        <img src="images/asu.avif" alt="ASU" style="width:100%; max-width:200px; border-radius:8px;">
      </div>
      <div class="paper-details">
        <h4>2018</h4>
        <ul>
          <li><i>Generalizing Graph Matching beyond Quadratic Assignment Model</i><br>
            Tianshu Yu, Junchi Yan, <strong>Yilin Wang</strong>, Wei Liu, Baoxin Li<br>
            NeurIPS 2018
          </li>
          <li><i>Weakly Supervised Facial Attribute Manipulation via Deep Adversarial Network</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Guojun Qi, Jiliang Tang, Baoxin Li<br>
            WACV 2018 <a href="papers/wacv18.pdf">[paper]</a>
          </li>
          <li><i>CrossFire: Cross Media Joint Friend and Item Recommendations</i><br>
            Kai Shu, Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            WSDM 2018 <span class="highlight">spotlight</span> <a href="https://dl.acm.org/citation.cfm?id=3159692">[paper]</a>
          </li>
          <li><i>Understanding and Predicting Delay in Reciprocal Relations</i><br>
            Jundong Li, Jiliang Tang, <strong>Yilin Wang</strong>, Yali Wan, Yi Chang, Huan Liu<br>
            WWW 2018 <span class="highlight">Research Track</span> <a href="https://arxiv.org/pdf/1703.01393.pdf">[arXiv]</a>
          </li>
          <li><i>Exploring Hierarchical Structures for Recommender Systems</i><br>
            Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            IEEE TKDE
          </li>
        </ul>
    
        <h4>2017</h4>
        <ul>
          <li><i>CLARE: A Joint Approach to Label Classification and Tag Recommendation</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Guojun Qi, Huan Liu, Baoxin Li<br>
            AAAI 2017 <span class="highlight">oral</span> <a href="papers/aaai_2017.pdf">[paper]</a> <a href="https://github.com/ywang370/ywang370.github.io/blob/master/admm_shoe.m">[code]</a>
          </li>
          <li><i>Understanding and Discovering Deliberate Self-harm Content in Social Media</i><br>
            <strong>Yilin Wang</strong>, Jiliang Tang, Jundong Li, Baoxin Li, Yali Wan, Clayton Mellina, Neil O'Hare, Yi Chang<br>
            WWW 2017 <span class="highlight">Research Track</span> <a href="papers/www_20171a.pdf">[paper]</a> <a href="papers/www2017.pdf">[slides]</a>
          </li>
          <li><i>Exploiting Hierarchical Structures for Unsupervised Feature Selection</i><br>
            Suhang Wang, <strong>Yilin Wang</strong>, Jiliang Tang, Charu Aggarwal, Suhas Ranganath, Huan Liu<br>
            SDM 2017 <a href="papers/sdm_2017.pdf">[paper]</a>
          </li>
          <li><i>What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest Recommendation</i><br>
            Suhang Wang, <strong>Yilin Wang</strong>, Jiliang Tang, Kai Shu, Suhas Ranganath, Huan Liu<br>
            WWW 2017 <span class="highlight">Research Track</span> <a href="papers/www_20171b.pdf">[paper]</a>
          </li>
        </ul>
    
        <h4>2016</h4>
        <ul>
          <li><i>PPP: Joint Pointwise and Pairwise Image Label Prediction</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Huan Liu, Baoxin Li<br>
            CVPR 2016 <a href="papers/PID4160349.pdf">[paper]</a>
          </li>
          <li><i>Efficient Unsupervised Abnormal Crowd Activity Detection Based on a Spatiotemporal Saliency Detector</i><br>
            <strong>Yilin Wang</strong>, Qiang Zhang, Baoxin Li<br>
            WACV 2016 <a href="papers/S3A_12.pdf">[paper]</a> <a href="https://github.com/ywang370/Video-Saliency">[code]</a>
          </li>
          <li><i>Scale Adaptive Eigen Eye for Fast Eye Detection in Wild Web Images</i><br>
            Xu Zhou, <strong>Yilin Wang</strong>, Peng Zhang, Baoxin Li<br>
            ICIP 2016
          </li>
        </ul>
    
        <h4>2015</h4>
        <ul>
          <li><i>Sentiment Analysis for Social Media Images</i><br>
            <strong>Yilin Wang</strong>, Baoxin Li<br>
            ICDM PhD Forum 2015
          </li>
          <li><i>Real Time Vehicle Back-up Warning System with Single Camera</i><br>
            <strong>Yilin Wang</strong>, Jun Cao, Baoxin Li<br>
            ICIP 2015 <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7351207&tag=1">[paper]</a>
          </li>
          <li><i>Unsupervised Sentiment Analysis for Social Media Images</i><br>
            <strong>Yilin Wang</strong>, Suhang Wang, Jiliang Tang, Huan Liu, Baoxin Li<br>
            IJCAI 2015 <a href="papers/Paper158_UESA.pdf">[paper]</a> <a href="projects/senti/senti.html">[project]</a>
          </li>
          <li><i>Inferring Sentiment from Web Images with Joint Inference on Visual and Social Cues: A Regulated Matrix Factorization Approach</i><br>
            <strong>Yilin Wang</strong>, Yuheng Hu, Subbarao Kambhampati, Baoxin Li<br>
            ICWSM 2015 <span class="highlight">oral</span> <a href="papers/latest_version_senti.pdf">[paper]</a>
          </li>
          <li><i>Structure Preserving Image Quality Assessment</i><br>
            <strong>Yilin Wang</strong>, Qiang Zhang, Baoxin Li<br>
            ICME 2015 <span class="highlight">oral</span> <a href="papers/271.pdf">[paper]</a>
          </li>
          <li><i>Exploring Implicit Hierarchical Structure for Recommender Systems</i><br>
            Suhang Wang, Jiliang Tang, <strong>Yilin Wang</strong>, Huan Liu<br>
            IJCAI 2015 <a href="papers/paper140.pdf">[paper]</a>
          </li>
          <li><i>Improving Vision-based Self-positioning in Intelligent Transportation Systems via Integrated Lane and Vehicle Detection</i><br>
            Parag S. Chandakkar, <strong>Yilin Wang</strong>, Baoxin Li<br>
            WACV 2015 <a href="papers/midas.pdf">[paper]</a>
          </li>
        </ul>
    
        <h4>2014</h4>
        <ul>
          <li><i>Image Co-segmentation via Multi-task Learning</i><br>
            Qiang Zhang, Jiayu Zhou, <strong>Yilin Wang</strong>, Jieping Ye, Baoxin Li<br>
            BMVC 2014 <a href="papers/bmvc2014.pdf">[paper]</a>
          </li>
        </ul>
      </div>
    </div>
    

    <h2>Service & Interns</h2>
    <ul>
      <li>Area Chair: ACM MM 2020, 2021</li>
      <li>Reviewer: CVPR, ICCV, ECCV, ICML, NeurIPS (since 2017)</li>
      <li>Interns collaborated with: <a href="https://zhke.io/"> Zhanghan Ke</a>,
        <a href="https://cs-people.bu.edu/nnli/">Nannan Li</a>,
        <a href="https://yiqunmei.net/">Yiqun Mei </a>,
        <br>
        <a href="https://yulunzhang.com/"> Yulun Zhang</a>,
        <a href="https://shikun.io//">Shikun Liu</a>,
        <a href="https://www.chenglinyang.com/">Chenglin Yang</a>,
        <br>
        <a href="https://jeya-maria-jose.github.io/research/">Jeya Maria Jose Valanarasu</a>,
        <a href="https://shikun.io//">Kenan E Ak</a>,
        <a href="https://yucornetto.github.io/">Qihang Yu</a>,
        <br>
        <a href="https://scholar.google.com/citations?user=EiD_2e0AAAAJ&hl=en">Xin Yuan </a>,
       <a href="https://yifanjiang19.github.io/">Yifan Jiang</a>,
       <a href="https://g-jing.github.io/">Jing Gu</a>,
       <a href="https://scholar.google.com.hk/citations?user=Q88BI2QAAAAJ&hl=en">Zhibo Yang </a>
    </ul>

    <footer class="text-center mt-5">
      <p style="font-size:small;">Website template leverage gpt4o and modified from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.</p>
    </footer>
  </div>
</body>
</html>
